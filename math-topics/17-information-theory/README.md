# Information Theory

## Entropy
H(X) = − Σ p(x) log p(x)

## Mutual Information
I(X;Y) = H(X) + H(Y) − H(X,Y)

## KL Divergence
D_KL(P||Q) = Σ P(x) log(P(x)/Q(x))

## Applications
- Data compression
- ML model uncertainty
